{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Data\n",
    "#X\n",
    "training_set_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]).T\n",
    "\n",
    "#Y\n",
    "training_set_outputs = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "\n",
    "#print (training_set_inputs.shape)\n",
    "#print (training_set_inputs)\n",
    "#print (training_set_outputs.shape)\n",
    "#print (training_set_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization Function (Weights and Bias)\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Function (Sigmoid in this case)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_and_backprop(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Forward pass cost function\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    # Compute Activation\n",
    "    A = sigmoid(np.dot(w.T,X)+b)                \n",
    "    #print (A)\n",
    "    \n",
    "    # Compute loss (The difference between the desired output and the predicted output)\n",
    "    # Then sum + extra to produce cost value\n",
    "    cost = -1 / m * np.sum(Y*np.log(A)+(1-Y)*np.log(1-A), axis = 1, keepdims = True)\n",
    "    \n",
    "        \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1 / m * np.dot(X,(A-Y).T)\n",
    "    db = 1 / m * np.sum(A-Y)\n",
    "    \n",
    "    \"\"\"\n",
    "    print (\"shape of A = \" + str(A.shape))\n",
    "    print (\"shape of Y = \" + str(Y.shape))\n",
    "    print (\"m = \" + str(m))\n",
    "    print (\"shape of dw = \" + str(dw.shape))\n",
    "    print (\"shape of w = \" + str(w.shape))\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "#Unit Test For forward_pass_and_backprop\n",
    "#cost = 5.801545319394553\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = forward_pass_and_backprop(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network adjusting the synaptic weights after each epoch\n",
    "def train(X, Y, number_of_training_iterations, w, b):\n",
    "        \n",
    "        print_cost = True\n",
    "        costs = []\n",
    "        \n",
    "        print (w)\n",
    "        \n",
    "        for iteration in range(number_of_training_iterations):\n",
    "        \n",
    "            # Pass the training set through our neural network (a single neuron) and compute cost\n",
    "            grads, cost = forward_pass_and_backprop(w, b, X, Y)\n",
    "\n",
    "            # Retrieve derivatives from grads\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "\n",
    "            # update weights\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            \n",
    "            # Record the costs\n",
    "            if iteration % 100 == 0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost and iteration % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(iteration, cost))\n",
    "                print (w)\n",
    "            \n",
    "            #print (w.T)\n",
    "                        \n",
    "        return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Cost after iteration 0: 0.693147\n",
      "[[0.0005]\n",
      " [0.    ]\n",
      " [0.    ]]\n",
      "Cost after iteration 100: 0.680942\n",
      "[[ 0.04988524]\n",
      " [-0.00030046]\n",
      " [-0.00060343]]\n",
      "Cost after iteration 200: 0.669274\n",
      "[[ 0.09811603]\n",
      " [-0.00113852]\n",
      " [-0.00229638]]\n",
      "Cost after iteration 300: 0.658070\n",
      "[[ 0.14527879]\n",
      " [-0.00243621]\n",
      " [-0.00493533]]\n",
      "Cost after iteration 400: 0.647273\n",
      "[[ 0.19145079]\n",
      " [-0.00412495]\n",
      " [-0.00839362]]\n",
      "Cost after iteration 500: 0.636838\n",
      "[[ 0.23670099]\n",
      " [-0.00614452]\n",
      " [-0.01255964]]\n",
      "Cost after iteration 600: 0.626728\n",
      "[[ 0.28109087]\n",
      " [-0.00844216]\n",
      " [-0.01733526]]\n",
      "Cost after iteration 700: 0.616915\n",
      "[[ 0.32467521]\n",
      " [-0.01097173]\n",
      " [-0.02263426]]\n",
      "Cost after iteration 800: 0.607374\n",
      "[[ 0.36750285]\n",
      " [-0.01369295]\n",
      " [-0.028381  ]]\n",
      "Cost after iteration 900: 0.598087\n",
      "[[ 0.40961737]\n",
      " [-0.01657068]\n",
      " [-0.03450916]]\n",
      "Cost after iteration 1000: 0.589037\n",
      "[[ 0.45105767]\n",
      " [-0.01957435]\n",
      " [-0.04096066]]\n",
      "Cost after iteration 1100: 0.580211\n",
      "[[ 0.49185856]\n",
      " [-0.02267737]\n",
      " [-0.04768467]]\n",
      "Cost after iteration 1200: 0.571599\n",
      "[[ 0.53205122]\n",
      " [-0.02585667]\n",
      " [-0.05463674]]\n",
      "Cost after iteration 1300: 0.563189\n",
      "[[ 0.57166366]\n",
      " [-0.02909227]\n",
      " [-0.061778  ]]\n",
      "Cost after iteration 1400: 0.554974\n",
      "[[ 0.61072106]\n",
      " [-0.03236689]\n",
      " [-0.06907453]]\n",
      "Cost after iteration 1500: 0.546946\n",
      "[[ 0.64924615]\n",
      " [-0.03566562]\n",
      " [-0.07649672]]\n",
      "Cost after iteration 1600: 0.539099\n",
      "[[ 0.68725952]\n",
      " [-0.03897563]\n",
      " [-0.08401876]]\n",
      "Cost after iteration 1700: 0.531426\n",
      "[[ 0.72477982]\n",
      " [-0.0422859 ]\n",
      " [-0.09161815]]\n",
      "Cost after iteration 1800: 0.523923\n",
      "[[ 0.76182406]\n",
      " [-0.04558703]\n",
      " [-0.09927532]]\n",
      "Cost after iteration 1900: 0.516583\n",
      "[[ 0.79840775]\n",
      " [-0.04887097]\n",
      " [-0.10697326]]\n",
      "Cost after iteration 2000: 0.509403\n",
      "[[ 0.83454513]\n",
      " [-0.05213091]\n",
      " [-0.11469718]]\n",
      "Cost after iteration 2100: 0.502377\n",
      "[[ 0.87024929]\n",
      " [-0.05536109]\n",
      " [-0.12243428]]\n",
      "Cost after iteration 2200: 0.495501\n",
      "[[ 0.90553232]\n",
      " [-0.05855669]\n",
      " [-0.13017346]]\n",
      "Cost after iteration 2300: 0.488772\n",
      "[[ 0.94040539]\n",
      " [-0.06171368]\n",
      " [-0.13790512]]\n",
      "Cost after iteration 2400: 0.482185\n",
      "[[ 0.97487895]\n",
      " [-0.06482875]\n",
      " [-0.14562101]]\n",
      "Cost after iteration 2500: 0.475736\n",
      "[[ 1.00896269]\n",
      " [-0.06789917]\n",
      " [-0.15331398]]\n",
      "Cost after iteration 2600: 0.469422\n",
      "[[ 1.04266572]\n",
      " [-0.07092278]\n",
      " [-0.16097795]]\n",
      "Cost after iteration 2700: 0.463240\n",
      "[[ 1.0759966 ]\n",
      " [-0.07389785]\n",
      " [-0.16860766]]\n",
      "Cost after iteration 2800: 0.457186\n",
      "[[ 1.10896339]\n",
      " [-0.07682307]\n",
      " [-0.17619868]]\n",
      "Cost after iteration 2900: 0.451257\n",
      "[[ 1.14157373]\n",
      " [-0.07969747]\n",
      " [-0.1837472 ]]\n",
      "Cost after iteration 3000: 0.445449\n",
      "[[ 1.17383487]\n",
      " [-0.08252037]\n",
      " [-0.19125005]]\n",
      "Cost after iteration 3100: 0.439760\n",
      "[[ 1.2057537 ]\n",
      " [-0.08529136]\n",
      " [-0.19870453]]\n",
      "Cost after iteration 3200: 0.434187\n",
      "[[ 1.23733681]\n",
      " [-0.08801023]\n",
      " [-0.20610841]]\n",
      "Cost after iteration 3300: 0.428726\n",
      "[[ 1.26859049]\n",
      " [-0.09067699]\n",
      " [-0.21345986]]\n",
      "Cost after iteration 3400: 0.423375\n",
      "[[ 1.2995208 ]\n",
      " [-0.09329178]\n",
      " [-0.22075737]]\n",
      "Cost after iteration 3500: 0.418131\n",
      "[[ 1.33013354]\n",
      " [-0.09585491]\n",
      " [-0.22799975]]\n",
      "Cost after iteration 3600: 0.412992\n",
      "[[ 1.36043432]\n",
      " [-0.09836679]\n",
      " [-0.23518604]]\n",
      "Cost after iteration 3700: 0.407954\n",
      "[[ 1.39042855]\n",
      " [-0.10082792]\n",
      " [-0.24231554]]\n",
      "Cost after iteration 3800: 0.403016\n",
      "[[ 1.42012146]\n",
      " [-0.1032389 ]\n",
      " [-0.24938773]]\n",
      "Cost after iteration 3900: 0.398174\n",
      "[[ 1.44951811]\n",
      " [-0.10560038]\n",
      " [-0.25640224]]\n",
      "Cost after iteration 4000: 0.393427\n",
      "[[ 1.47862342]\n",
      " [-0.10791309]\n",
      " [-0.26335889]]\n",
      "Cost after iteration 4100: 0.388772\n",
      "[[ 1.50744217]\n",
      " [-0.11017779]\n",
      " [-0.27025758]]\n",
      "Cost after iteration 4200: 0.384207\n",
      "[[ 1.53597901]\n",
      " [-0.11239528]\n",
      " [-0.27709837]]\n",
      "Cost after iteration 4300: 0.379730\n",
      "[[ 1.56423845]\n",
      " [-0.11456638]\n",
      " [-0.28388137]]\n",
      "Cost after iteration 4400: 0.375338\n",
      "[[ 1.59222491]\n",
      " [-0.11669196]\n",
      " [-0.2906068 ]]\n",
      "Cost after iteration 4500: 0.371030\n",
      "[[ 1.61994268]\n",
      " [-0.11877287]\n",
      " [-0.29727494]]\n",
      "Cost after iteration 4600: 0.366803\n",
      "[[ 1.64739596]\n",
      " [-0.12080999]\n",
      " [-0.30388613]]\n",
      "Cost after iteration 4700: 0.362655\n",
      "[[ 1.67458886]\n",
      " [-0.12280422]\n",
      " [-0.31044076]]\n",
      "Cost after iteration 4800: 0.358586\n",
      "[[ 1.70152538]\n",
      " [-0.12475643]\n",
      " [-0.31693927]]\n",
      "Cost after iteration 4900: 0.354591\n",
      "[[ 1.72820945]\n",
      " [-0.12666751]\n",
      " [-0.32338214]]\n",
      "Cost after iteration 5000: 0.350671\n",
      "[[ 1.7546449 ]\n",
      " [-0.12853834]\n",
      " [-0.32976986]]\n",
      "Cost after iteration 5100: 0.346823\n",
      "[[ 1.78083549]\n",
      " [-0.13036981]\n",
      " [-0.33610297]]\n",
      "Cost after iteration 5200: 0.343045\n",
      "[[ 1.80678489]\n",
      " [-0.13216278]\n",
      " [-0.34238201]]\n",
      "Cost after iteration 5300: 0.339336\n",
      "[[ 1.83249673]\n",
      " [-0.1339181 ]\n",
      " [-0.34860755]]\n",
      "Cost after iteration 5400: 0.335694\n",
      "[[ 1.85797451]\n",
      " [-0.13563663]\n",
      " [-0.35478017]]\n",
      "Cost after iteration 5500: 0.332118\n",
      "[[ 1.88322172]\n",
      " [-0.13731921]\n",
      " [-0.36090046]]\n",
      "Cost after iteration 5600: 0.328606\n",
      "[[ 1.90824175]\n",
      " [-0.13896666]\n",
      " [-0.36696901]]\n",
      "Cost after iteration 5700: 0.325156\n",
      "[[ 1.93303793]\n",
      " [-0.14057978]\n",
      " [-0.37298644]]\n",
      "Cost after iteration 5800: 0.321767\n",
      "[[ 1.95761352]\n",
      " [-0.14215937]\n",
      " [-0.37895335]]\n",
      "Cost after iteration 5900: 0.318438\n",
      "[[ 1.98197174]\n",
      " [-0.14370621]\n",
      " [-0.38487035]]\n",
      "Cost after iteration 6000: 0.315167\n",
      "[[ 2.00611573]\n",
      " [-0.14522107]\n",
      " [-0.39073804]]\n",
      "Cost after iteration 6100: 0.311953\n",
      "[[ 2.03004858]\n",
      " [-0.14670469]\n",
      " [-0.39655705]]\n",
      "Cost after iteration 6200: 0.308795\n",
      "[[ 2.05377331]\n",
      " [-0.14815781]\n",
      " [-0.40232796]]\n",
      "Cost after iteration 6300: 0.305691\n",
      "[[ 2.07729292]\n",
      " [-0.14958114]\n",
      " [-0.40805139]]\n",
      "Cost after iteration 6400: 0.302640\n",
      "[[ 2.10061031]\n",
      " [-0.15097539]\n",
      " [-0.41372793]]\n",
      "Cost after iteration 6500: 0.299641\n",
      "[[ 2.12372837]\n",
      " [-0.15234124]\n",
      " [-0.41935819]]\n",
      "Cost after iteration 6600: 0.296693\n",
      "[[ 2.14664989]\n",
      " [-0.15367935]\n",
      " [-0.42494274]]\n",
      "Cost after iteration 6700: 0.293794\n",
      "[[ 2.16937766]\n",
      " [-0.1549904 ]\n",
      " [-0.43048219]]\n",
      "Cost after iteration 6800: 0.290944\n",
      "[[ 2.19191439]\n",
      " [-0.156275  ]\n",
      " [-0.43597709]]\n",
      "Cost after iteration 6900: 0.288141\n",
      "[[ 2.21426275]\n",
      " [-0.15753378]\n",
      " [-0.44142803]]\n",
      "Cost after iteration 7000: 0.285385\n",
      "[[ 2.23642536]\n",
      " [-0.15876736]\n",
      " [-0.44683558]]\n",
      "Cost after iteration 7100: 0.282674\n",
      "[[ 2.25840479]\n",
      " [-0.15997633]\n",
      " [-0.45220028]]\n",
      "Cost after iteration 7200: 0.280008\n",
      "[[ 2.28020358]\n",
      " [-0.16116125]\n",
      " [-0.45752269]]\n",
      "Cost after iteration 7300: 0.277384\n",
      "[[ 2.30182421]\n",
      " [-0.16232271]\n",
      " [-0.46280336]]\n",
      "Cost after iteration 7400: 0.274804\n",
      "[[ 2.32326913]\n",
      " [-0.16346124]\n",
      " [-0.46804283]]\n",
      "Cost after iteration 7500: 0.272264\n",
      "[[ 2.34454073]\n",
      " [-0.16457738]\n",
      " [-0.47324161]]\n",
      "Cost after iteration 7600: 0.269766\n",
      "[[ 2.36564136]\n",
      " [-0.16567166]\n",
      " [-0.47840024]]\n",
      "Cost after iteration 7700: 0.267307\n",
      "[[ 2.38657335]\n",
      " [-0.16674458]\n",
      " [-0.48351923]]\n",
      "Cost after iteration 7800: 0.264887\n",
      "[[ 2.40733898]\n",
      " [-0.16779664]\n",
      " [-0.48859908]]\n",
      "Cost after iteration 7900: 0.262505\n",
      "[[ 2.42794047]\n",
      " [-0.16882833]\n",
      " [-0.4936403 ]]\n",
      "Cost after iteration 8000: 0.260161\n",
      "[[ 2.44838003]\n",
      " [-0.16984011]\n",
      " [-0.49864338]]\n",
      "Cost after iteration 8100: 0.257852\n",
      "[[ 2.46865982]\n",
      " [-0.17083245]\n",
      " [-0.50360879]]\n",
      "Cost after iteration 8200: 0.255580\n",
      "[[ 2.48878195]\n",
      " [-0.17180579]\n",
      " [-0.50853703]]\n",
      "Cost after iteration 8300: 0.253343\n",
      "[[ 2.50874853]\n",
      " [-0.17276056]\n",
      " [-0.51342856]]\n",
      "Cost after iteration 8400: 0.251139\n",
      "[[ 2.52856159]\n",
      " [-0.1736972 ]\n",
      " [-0.51828384]]\n",
      "Cost after iteration 8500: 0.248970\n",
      "[[ 2.54822315]\n",
      " [-0.17461611]\n",
      " [-0.52310332]]\n",
      "Cost after iteration 8600: 0.246833\n",
      "[[ 2.5677352 ]\n",
      " [-0.1755177 ]\n",
      " [-0.52788746]]\n",
      "Cost after iteration 8700: 0.244728\n",
      "[[ 2.58709969]\n",
      " [-0.17640237]\n",
      " [-0.5326367 ]]\n",
      "Cost after iteration 8800: 0.242655\n",
      "[[ 2.60631852]\n",
      " [-0.17727049]\n",
      " [-0.53735146]]\n",
      "Cost after iteration 8900: 0.240613\n",
      "[[ 2.62539358]\n",
      " [-0.17812243]\n",
      " [-0.54203219]]\n",
      "Cost after iteration 9000: 0.238601\n",
      "[[ 2.64432672]\n",
      " [-0.17895857]\n",
      " [-0.5466793 ]]\n",
      "Cost after iteration 9100: 0.236619\n",
      "[[ 2.66311977]\n",
      " [-0.17977925]\n",
      " [-0.5512932 ]]\n",
      "Cost after iteration 9200: 0.234665\n",
      "[[ 2.6817745 ]\n",
      " [-0.18058483]\n",
      " [-0.5558743 ]]\n",
      "Cost after iteration 9300: 0.232740\n",
      "[[ 2.70029268]\n",
      " [-0.18137562]\n",
      " [-0.560423  ]]\n",
      "Cost after iteration 9400: 0.230843\n",
      "[[ 2.71867605]\n",
      " [-0.18215198]\n",
      " [-0.5649397 ]]\n",
      "Cost after iteration 9500: 0.228974\n",
      "[[ 2.73692629]\n",
      " [-0.1829142 ]\n",
      " [-0.56942478]]\n",
      "Cost after iteration 9600: 0.227131\n",
      "[[ 2.75504508]\n",
      " [-0.18366261]\n",
      " [-0.57387864]]\n",
      "Cost after iteration 9700: 0.225314\n",
      "[[ 2.77303407]\n",
      " [-0.18439751]\n",
      " [-0.57830163]]\n",
      "Cost after iteration 9800: 0.223524\n",
      "[[ 2.79089487]\n",
      " [-0.18511918]\n",
      " [-0.58269414]]\n",
      "Cost after iteration 9900: 0.221758\n",
      "[[ 2.80862908]\n",
      " [-0.18582793]\n",
      " [-0.58705653]]\n"
     ]
    }
   ],
   "source": [
    "#Train the Network (Model)\n",
    "\n",
    "# Base Hyperparameters\n",
    "number_of_training_iterations = 10000\n",
    "learning_rate = 0.002\n",
    "\n",
    "# initialize parameters\n",
    "X = training_set_inputs\n",
    "Y = training_set_outputs\n",
    "w, b = initialize_with_zeros(X.shape[0])\n",
    "\n",
    "# Train\n",
    "w,b = train(X, Y, number_of_training_iterations, w, b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[0.90332406]\n",
      "[1]\n",
      "[[ 2.82606278]\n",
      " [-0.18651714]\n",
      " [-0.59134598]]\n",
      "\n",
      "Considering new situation [0, 0, 0] -> ?: \n",
      "[0.35632608]\n",
      "[0]\n",
      "[[ 2.82606278]\n",
      " [-0.18651714]\n",
      " [-0.59134598]]\n"
     ]
    }
   ],
   "source": [
    "#Try a Test Value\n",
    "print (\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "test = np.array([1, 0, 0])\n",
    "yhat = sigmoid(np.dot(w.T,test)+b)\n",
    "print (yhat)\n",
    "print (np.where(yhat>0.5,1,0))\n",
    "print (w)\n",
    "\n",
    "print ('')\n",
    "print (\"Considering new situation [0, 0, 0] -> ?: \")\n",
    "test = np.array([0, 0, 0])\n",
    "yhat = sigmoid(np.dot(w.T,test)+b)\n",
    "print (yhat)\n",
    "print (np.where(yhat>0.5,1,0))\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
