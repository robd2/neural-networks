{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Data\n",
    "#X\n",
    "training_set_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]).T\n",
    "\n",
    "#Y\n",
    "training_set_outputs = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "\n",
    "#print (training_set_inputs.shape)\n",
    "#print (training_set_inputs)\n",
    "#print (training_set_outputs.shape)\n",
    "#print (training_set_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization Function (Weights and Bias)\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Function (Sigmoid in this case)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_and_backprop(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Forward pass cost function\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    # Compute Activation\n",
    "    A = sigmoid(np.dot(w.T,X)+b)                \n",
    "    #print (A)\n",
    "    \n",
    "    # Compute loss (The difference between the desired output and the predicted output)\n",
    "    # Then sum + extra to produce cost value\n",
    "    cost = -1 / m * np.sum(Y*np.log(A)+(1-Y)*np.log(1-A), axis = 1, keepdims = True)\n",
    "    \n",
    "        \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1 / m * np.dot(X,(A-Y).T)\n",
    "    db = 1 / m * np.sum(A-Y)\n",
    "    \n",
    "    \"\"\"\n",
    "    print (\"shape of A = \" + str(A.shape))\n",
    "    print (\"shape of Y = \" + str(Y.shape))\n",
    "    print (\"m = \" + str(m))\n",
    "    print (\"shape of dw = \" + str(dw.shape))\n",
    "    print (\"shape of w = \" + str(w.shape))\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "#Unit Test For forward_pass_and_backprop\n",
    "#cost = 5.801545319394553\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = forward_pass_and_backprop(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network adjusting the synaptic weights after each epoch\n",
    "def train(X, Y, number_of_training_iterations, w, b):\n",
    "        \n",
    "        print_cost = True\n",
    "        costs = []\n",
    "        \n",
    "        for iteration in range(number_of_training_iterations):\n",
    "        \n",
    "            # Pass the training set through our neural network (a single neuron) and compute cost\n",
    "            grads, cost = forward_pass_and_backprop(w, b, X, Y)\n",
    "\n",
    "            # Retrieve derivatives from grads\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "\n",
    "            # update weights\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            \n",
    "            # Record the costs\n",
    "            if iteration % 100 == 0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost and iteration % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(iteration, cost))\n",
    "            \n",
    "            \n",
    "            #print (w.T)\n",
    "                        \n",
    "        return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.680942\n",
      "Cost after iteration 200: 0.669274\n",
      "Cost after iteration 300: 0.658070\n",
      "Cost after iteration 400: 0.647273\n",
      "Cost after iteration 500: 0.636838\n",
      "Cost after iteration 600: 0.626728\n",
      "Cost after iteration 700: 0.616915\n",
      "Cost after iteration 800: 0.607374\n",
      "Cost after iteration 900: 0.598087\n",
      "Cost after iteration 1000: 0.589037\n",
      "Cost after iteration 1100: 0.580211\n",
      "Cost after iteration 1200: 0.571599\n",
      "Cost after iteration 1300: 0.563189\n",
      "Cost after iteration 1400: 0.554974\n",
      "Cost after iteration 1500: 0.546946\n",
      "Cost after iteration 1600: 0.539099\n",
      "Cost after iteration 1700: 0.531426\n",
      "Cost after iteration 1800: 0.523923\n",
      "Cost after iteration 1900: 0.516583\n",
      "Cost after iteration 2000: 0.509403\n",
      "Cost after iteration 2100: 0.502377\n",
      "Cost after iteration 2200: 0.495501\n",
      "Cost after iteration 2300: 0.488772\n",
      "Cost after iteration 2400: 0.482185\n",
      "Cost after iteration 2500: 0.475736\n",
      "Cost after iteration 2600: 0.469422\n",
      "Cost after iteration 2700: 0.463240\n",
      "Cost after iteration 2800: 0.457186\n",
      "Cost after iteration 2900: 0.451257\n",
      "Cost after iteration 3000: 0.445449\n",
      "Cost after iteration 3100: 0.439760\n",
      "Cost after iteration 3200: 0.434187\n",
      "Cost after iteration 3300: 0.428726\n",
      "Cost after iteration 3400: 0.423375\n",
      "Cost after iteration 3500: 0.418131\n",
      "Cost after iteration 3600: 0.412992\n",
      "Cost after iteration 3700: 0.407954\n",
      "Cost after iteration 3800: 0.403016\n",
      "Cost after iteration 3900: 0.398174\n",
      "Cost after iteration 4000: 0.393427\n",
      "Cost after iteration 4100: 0.388772\n",
      "Cost after iteration 4200: 0.384207\n",
      "Cost after iteration 4300: 0.379730\n",
      "Cost after iteration 4400: 0.375338\n",
      "Cost after iteration 4500: 0.371030\n",
      "Cost after iteration 4600: 0.366803\n",
      "Cost after iteration 4700: 0.362655\n",
      "Cost after iteration 4800: 0.358586\n",
      "Cost after iteration 4900: 0.354591\n",
      "Cost after iteration 5000: 0.350671\n",
      "Cost after iteration 5100: 0.346823\n",
      "Cost after iteration 5200: 0.343045\n",
      "Cost after iteration 5300: 0.339336\n",
      "Cost after iteration 5400: 0.335694\n",
      "Cost after iteration 5500: 0.332118\n",
      "Cost after iteration 5600: 0.328606\n",
      "Cost after iteration 5700: 0.325156\n",
      "Cost after iteration 5800: 0.321767\n",
      "Cost after iteration 5900: 0.318438\n",
      "Cost after iteration 6000: 0.315167\n",
      "Cost after iteration 6100: 0.311953\n",
      "Cost after iteration 6200: 0.308795\n",
      "Cost after iteration 6300: 0.305691\n",
      "Cost after iteration 6400: 0.302640\n",
      "Cost after iteration 6500: 0.299641\n",
      "Cost after iteration 6600: 0.296693\n",
      "Cost after iteration 6700: 0.293794\n",
      "Cost after iteration 6800: 0.290944\n",
      "Cost after iteration 6900: 0.288141\n",
      "Cost after iteration 7000: 0.285385\n",
      "Cost after iteration 7100: 0.282674\n",
      "Cost after iteration 7200: 0.280008\n",
      "Cost after iteration 7300: 0.277384\n",
      "Cost after iteration 7400: 0.274804\n",
      "Cost after iteration 7500: 0.272264\n",
      "Cost after iteration 7600: 0.269766\n",
      "Cost after iteration 7700: 0.267307\n",
      "Cost after iteration 7800: 0.264887\n",
      "Cost after iteration 7900: 0.262505\n",
      "Cost after iteration 8000: 0.260161\n",
      "Cost after iteration 8100: 0.257852\n",
      "Cost after iteration 8200: 0.255580\n",
      "Cost after iteration 8300: 0.253343\n",
      "Cost after iteration 8400: 0.251139\n",
      "Cost after iteration 8500: 0.248970\n",
      "Cost after iteration 8600: 0.246833\n",
      "Cost after iteration 8700: 0.244728\n",
      "Cost after iteration 8800: 0.242655\n",
      "Cost after iteration 8900: 0.240613\n",
      "Cost after iteration 9000: 0.238601\n",
      "Cost after iteration 9100: 0.236619\n",
      "Cost after iteration 9200: 0.234665\n",
      "Cost after iteration 9300: 0.232740\n",
      "Cost after iteration 9400: 0.230843\n",
      "Cost after iteration 9500: 0.228974\n",
      "Cost after iteration 9600: 0.227131\n",
      "Cost after iteration 9700: 0.225314\n",
      "Cost after iteration 9800: 0.223524\n",
      "Cost after iteration 9900: 0.221758\n"
     ]
    }
   ],
   "source": [
    "#Train the Network (Model)\n",
    "\n",
    "# Base Hyperparameters\n",
    "number_of_training_iterations = 10000\n",
    "learning_rate = 0.002\n",
    "\n",
    "# initialize parameters\n",
    "X = training_set_inputs\n",
    "Y = training_set_outputs\n",
    "w, b = initialize_with_zeros(X.shape[0])\n",
    "\n",
    "# Train\n",
    "w,b = train(X, Y, number_of_training_iterations, w, b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[1]\n",
      "[[ 2.82606278]\n",
      " [-0.18651714]\n",
      " [-0.59134598]]\n"
     ]
    }
   ],
   "source": [
    "#Try a Test Value\n",
    "print (\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "test = np.array([1, 0, 0])\n",
    "yhat = sigmoid(np.dot(w.T,test)+b)\n",
    "print (np.where(yhat>0.5,1,0))\n",
    "\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
